{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4d2c5b-a4a2-4d69-9534-8cfaafa5a1f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q .1 What is a parameter?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459e31ae-cddf-43c3-af36-dfd4baa6f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Parameters is define how raw data is transformed into features suitable for machine learning.\n",
    "\n",
    "# >> It is a variable that is used to control the behavior of function , parameters are used in mathematics, computer programming , to define or constain a process.\n",
    "\n",
    "# >> Parameters are fixed inputs that influence the behaviour or output of a system.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c9a6f-41cc-4ea0-b03d-b3763b07c40a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q 2 What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31653347-9fcf-44eb-9b68-97767248da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Coreletion is a statistical reletion between two variable.\n",
    "# >> correlation helps in identifing the irrelevant features.\n",
    "# FEATURES:\n",
    "# >> positive correlation :-\n",
    "#   when both variable increase together.'\n",
    "\n",
    "# >> Negetive correletion :-\n",
    "#   when one variable increase while other decrease.\n",
    "\n",
    "# >> No correletion :-\n",
    "#   no linear reletionship, variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08db8f3-dab4-49e3-817d-c967723870f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q.3 Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f64c8d-7ea6-450e-91fc-90e3f6acc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  >> Machine learning is a subset of artifitial intelligence that enables systems to automatically learn from data and improve their performance on specific task without being explictly programmed.\n",
    "\n",
    "#  >> It uses algorithms to identify patterns , make decisions and predict outcomes based on data.\n",
    "\n",
    "#  MAIN COMPONENTS OF MACHINE LEARNING  \n",
    "\n",
    "# ** Data :-\n",
    "# It includes raw information from various sources.]\n",
    "# types :-\n",
    "# > Structured Data\n",
    "# > Unstructured Data\n",
    "# ** Features :-\n",
    "# Input variable that used by th model to make prediction.\n",
    "# Feature engineering transform raw data into relevent inputs.\n",
    "# ** model :\n",
    "# Mathematical representation of a process that maps inputs t outputs.\n",
    "\n",
    "# ** Algorithm :-\n",
    "# step by step procedure or formulas that model used to learn the pattern.\n",
    "\n",
    "# ** Optimization :-\n",
    "# Techniques to improve model performance by minimizing the error.\n",
    "\n",
    "# machine learning success depends on effectively combining these components to solve real world programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ea598-981e-4c26-aa83-ee5623cb9af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q.4 How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cfd6e7d-40c7-4d45-8234-0b6797c5d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its a critical metric in ML that help in determining how well a model is performing during training.\n",
    "\n",
    "# how it determine :\n",
    "# >> measures error :-\n",
    "#     The loss function quantifies the difference between the model predictions and the true target values.\n",
    "# >> Guides optimization :-\n",
    "#     During training the loss value used to adjust the models parameters via optimization algorithms like gradient descent.\n",
    "# >> Detect overfitting and underfitting :-\n",
    "#     overfitting :-\n",
    "#                 if training loss is low and validation loss is high. the model may be overfitting.\n",
    "#     underfitting :-\n",
    "#                 If both traing and validation losses are high the model is likely underfitting.\n",
    "\n",
    "# >> Model Selection :-\n",
    "#     loss function also select on the basis of of the type of task. A well chosen loss function reflects how closly the model aligns with the problems goals.\n",
    "\n",
    "# >>  Comparision between the Model :=\n",
    "#     loss values allow for direct comparisions between different models or configuration.model with lowest loss in validation considered as best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7990f-29a1-4935-aec9-54084a071e5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q.5 What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b39f704-54c1-4c85-a091-9634bde24de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In statistic we catogarized variable in two types 1. continuous variable\n",
    "#                                                   2. categorical variable\n",
    "# Continuous variable :-\n",
    "#                  In a range varialbles that takes an infinite numbers of values is continuous variable.\n",
    "\n",
    "#  example :-  ..Height\n",
    "#              ..Weight\n",
    "# categoriacal variable :-\n",
    "#                  variable that represents the represent discrete categories.\n",
    "\n",
    "#  example :- .. Gender\n",
    "#             .. Colour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911ee0e-f88f-42c1-b031-b9763bb1aa89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q.6 How do we handle categorical variables in Machine Learning? What are the common techniques?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2969f0f3-2556-4750-9e42-dcc0e71230d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handlig involves converting variables in a numerical format that algoriths can process.\n",
    "bot \n",
    "#  TECHNIQUES :\n",
    "# 1, Lable encoding :-\n",
    "#                 Assigns a unique integer to each category.\n",
    "\n",
    "# 2,One-Hot Encoding :-\n",
    "#                 creates binary columns for each categories.\n",
    "\n",
    "# 3,Ordinal Encoding :-\n",
    "#                 Assigns orddered integers based on the category's rank .\n",
    "\n",
    "# 4,Frequency Encoding :-\n",
    "#                 Encoding categories  based on their frequency in the dataset.\n",
    "\n",
    "# 5, Embedding Encoding :-\n",
    "#                 Learns dense vector representations for ctegories, often used woth neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35705d-580a-436d-9969-912acd0f085a",
   "metadata": {},
   "source": [
    " # Q.7 What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f1c958c-6101-4fea-b929-9657ea37229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both are essential for valiadting the machine learning model.\n",
    "\n",
    "# 1.Training a dataset :-\n",
    "#                   the precess of using a subset of data to teach the machine learning model.\n",
    "\n",
    "#                   It use to enable the model to make prediction based on given input.\n",
    "# 2. Testing a dataset :-\n",
    "#                   The process of evaluating the trained model's performance on a different subset of the data.\n",
    "\n",
    "#                   used to access the models ability to generalise learning to new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422354c-2c31-4d50-af35-d54bb6fd7d57",
   "metadata": {},
   "source": [
    "# Q.8 What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9044a50d-6e75-4363-b315-ef09c894ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its a collection of technique to convert the raw data into a formate suitable for the machine learning model.\n",
    "\n",
    "# key features :\n",
    "\n",
    "# 1.scaling\n",
    "# 2.Encoding\n",
    "# 3.Normalizing(ensure consistent data range )\n",
    "# 4.Feature generation(create features for better learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766e414-b881-471e-837e-230922d14bee",
   "metadata": {},
   "source": [
    "# Q. 9 What is a Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "412e264c-23cc-450a-b2d0-0e941ad7e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Its a subset of dataset used to evaluatethe performance of the trained machine learning model.\n",
    "# >> It represent new , unseen data that the model has not encountered during training process.\n",
    "\n",
    "# CHARACTERISTIC:=\n",
    "\n",
    "# 1. seperation of training data :-\n",
    "#                               to ensure unbiased evaluation traing and test set must not overlap.\n",
    "\n",
    "# 2.Purpose :=\n",
    "#           Used exclusivly for performance evaluation after the model has been trained.\n",
    "\n",
    "# 3.Size := \n",
    "#        typically comprises of 20-30% of the total dataset.\n",
    "\n",
    "# importance :\n",
    "\n",
    "# 1.validation of model performance.\n",
    "# 2. Avoid Overfitting.\n",
    "# 3. Reliable metrics(provide unbiased evaluation of metric like accuracy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406eb1e-9f89-4afa-8eb5-a20f52e23518",
   "metadata": {},
   "source": [
    "# Q.10 How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218a9b27-dce7-4ca5-8738-8f9001f9219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproach to machine learnng problem :\n",
    "# 1, Understanding the problem :-\n",
    "#                             >> defines the problem statement.\n",
    "#                             >> Identify the target variable and types of problem.\n",
    "# 2,Gather and explore data :-\n",
    "#                             >> Load the dataset.\n",
    "#                             >> perform EDA using a tools like pandas.\n",
    "#                             >> check for missing values.\n",
    "# 3,Data preprocessing :-\n",
    "#                             >> Handle missing values\n",
    "\n",
    "# 4,Split the dataset :-\n",
    "#                             >> Divide the data into training, validation and test sets using train_test_split.\n",
    "# 5,Feature engineering :-\n",
    "#                             >> Create new features or transform existing one if it improves model performance.\n",
    "# 6, Select the model :-\n",
    "#                               >> based on problem types\n",
    "# 7,TRain the model :\n",
    "\n",
    "# 8, evaluate the model(using matrics)\n",
    "# 9, Hyperparameter tuning :-\n",
    "#                              >> Optimize the model using techniques like grid search or random search.\n",
    "# 10,Test the model :\n",
    "\n",
    "# 11, Deploy and monitor : (deploy the model to production and monitor the performance over time.)\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826970f-90ee-4ec2-a0bc-f9b22c8f9130",
   "metadata": {},
   "source": [
    "# Q.11 Why do we have to perform EDA before fitting a model to the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e59d81b-107b-41c7-b250-f35dc8bf11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It helps to ensure the quality , relevence and suitability of the dataset for modeling.\n",
    "\n",
    "#             ** IMPORTANCE **\n",
    "# 1 , Understanding the Dataset :-\n",
    "#                              identifying the data structure by understanding the type of variables.\n",
    "# 2 , Handle missing data :-\n",
    "#                             missing value leads to error, EDA helps to identify the missing data so that you can decide wheter to \n",
    "#                                                                          >>impute \n",
    "#                                                                          >> drop \n",
    "#                                                                          >> or use algorithm that handel missing data natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540f30c-38bf-42a4-be4f-48b76a3e9d79",
   "metadata": {},
   "source": [
    "# Q.12 What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af928334-bea8-4522-a547-c9c6d1180d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..Its a statistical measure that quantifies the reletionship between two variable.\n",
    "# ..It is expressed as a value between -1 and 1.\n",
    "# >> types of correletion ;\n",
    "# ~ Positive correletion\n",
    "# ~ Negetive correletion\n",
    "# ~ Zero correletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f8ee5-7574-497f-8e6d-42650fbc285f",
   "metadata": {},
   "source": [
    " # Q.13 What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9df7787-115a-4ab2-9795-f0d58e3bcc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an one variable increase , the other variable decrease and vice versa.\n",
    "# Real world Application :-\n",
    "# 1.physics :- speed increase the time taken decrease.\n",
    "# 2. Economy :-higher the unemployment , lower the consumer spending ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b51953-1039-4e2e-9e14-f63c9b0ef41a",
   "metadata": {},
   "source": [
    "# Q.14 How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217b7b76-d8b8-453a-8692-4efbdbd8eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for finding a correletion between variable in pythons.\n",
    "# 1. using numpy :-\n",
    "#                 by using  np.corrcoef()\n",
    "# 2. using pandas :-\n",
    "#                 by using Dataframe.corr()\n",
    "# 3. Using scipy :-\n",
    "#                 by scipy.stats module.\n",
    "# 4. Visualizing Correlation :-\n",
    "#                 to understand correletions visually,, you can use a heatmap with seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4e911-d58d-4c0b-810e-c80578cad093",
   "metadata": {},
   "source": [
    "# Q. 15 What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b391165-d959-441e-b54c-93d66f134552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>It implies that cahnge in one variable produce change in another.\n",
    "# >>while correletion means the statistical measure that describes the extent to which two variable move together.\n",
    "\n",
    "#     KEY DIFFERENCES :=\n",
    "# 1. causation :- implies a cause and effective relationship.\n",
    "#    correlation:- implies only that two variables are related or move together.\n",
    "# 2. >> causation requirs evidences and reasoning to prove a direct connection.\n",
    "#    >> correlation :- DOes not requir proof of one variable influencing the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52501c9e-7ee1-4407-96c8-352ab1513a66",
   "metadata": {},
   "source": [
    "# Q .16 What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04940b5c-e293-4182-a436-a32ba624b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer in the machine leaarning is a algorithm used to adjust the parametersof the model to minimize the loos function \n",
    "# and improve its performance.\n",
    "#     types :=\n",
    "# 1. Gradient Descent :-\n",
    "# calculate the gradient of the loos function concerning all the parameters using entire training dataset.\n",
    "\n",
    "# example ;= \n",
    "# trainig a model for small dataset\n",
    "# 2. stochastic Gradient Dataset :-\n",
    "# SGD updates parameters based on a single data point ata time.\n",
    "\n",
    "# example :=\n",
    "# Training a neural network on a massive dataset like CIFAR-10\n",
    "# 3. momentum :-\n",
    "# it incoorporates the previous updates direction to smooth the updates\n",
    "# 4. Adagrad :-Adagrad adopts the learning rate for each parameter based on the sum of all past gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66191f7d-9f06-407a-bd7b-3ba8f255c0f7",
   "metadata": {},
   "source": [
    "# Q .17 What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7e8bd92-8858-467b-88cc-f1f1056aec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its a module in scikit-learn library that provides a collection of algorithms for solving linear problems in machine learning.\n",
    "#     FEATUREs :=\n",
    "# 1, assumes a linear reletionship between input feature and target variable.\n",
    "# 2, Includes models for simple and complex linear problems.\n",
    "# 3, Support both regularized and unregularized version of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0036ba-6e7f-4458-958f-7070d9badfe5",
   "metadata": {},
   "source": [
    "# Q.18 What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c039df-8cd4-4de9-a1a2-8bd08b315db6",
   "metadata": {},
   "outputs": [],
   "source": [
    ">> the model.fit() function is used to train a model on a dataset.\n",
    ">> It adjusts the model's internal parameters (weights) by minimizing the loss function based on the provided training data.\n",
    "\n",
    " Key Arguments :=\n",
    "\n",
    ">> The input data used for training the model.\n",
    ">> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aed1d0-6730-443a-b00a-741bc712c50f",
   "metadata": {},
   "source": [
    "# Q . 19 What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776202b-52e0-4a02-9b52-5f12b77dbea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01e9f3a5-5bbb-41a5-b618-d6a74b6187ab",
   "metadata": {},
   "source": [
    "# Q . 20 What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc52fff-6c26-4093-8b45-82408f28fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE are type of data used in statistics , machine learning .\n",
    "\n",
    "# >> Continuous variable :-\n",
    "#  continuous variable takes an infinit number of values within a given range .\n",
    "# >> Categorical variable :-\n",
    "#  categorical variables represent discrete groups.\n",
    "#  They usually describe quality and characteristics that cant be measured numerically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b774388-60ec-4156-8fbd-9144b29b107d",
   "metadata": {},
   "source": [
    "# Q. 21 What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b0543a7-697c-4eeb-81b8-37a81cc9297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling is the process of normalising the range of independent variablesin a dataset.\n",
    "\n",
    "#     importance in machine learning :-\n",
    "# 1, Improve model performance \n",
    "# 2, Speedup convergence ; \n",
    "# 3, avoids dominance by larger values .\n",
    "# 4, Enables Fair comparisons.\n",
    "\n",
    "# ~ feature scaling is neccessary for distance based algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf72695-d000-45ae-9853-56f80bee1880",
   "metadata": {},
   "source": [
    "# Q. 22 How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf272c7-315f-4310-8eec-9f7aada1edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pyhton feature scaling is performed using the scikit-learn library. which provide convenent tools for various scaling technique'\n",
    "# METHODS ;\n",
    "# >Normalizing \n",
    "# >Standarizing\n",
    "# >Robust scaling\n",
    "# > Manual scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae44ba-5be8-43bb-9b76-fe1c9872ea41",
   "metadata": {},
   "source": [
    "# Q.23 What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbcd1d-bc35-4175-baa5-f86310e90fde",
   "metadata": {},
   "outputs": [],
   "source": [
    ">> its a module in scikit-learn provides tools and techniques for data preprocessing .\n",
    ">> processing is important in machine learning for transforming the data.\n",
    "\n",
    "    key features ;\n",
    "1, scaling \n",
    "2, Normalizing\n",
    "3,Encoding categoriacal data.\n",
    "4,Binarization\n",
    "5,polynomial Features\n",
    "6, Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ee336-a918-4498-a33d-f5ef58b2af9a",
   "metadata": {},
   "source": [
    "# Q .24 How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b20555-afaf-4ecd-b696-d8df50c522ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common method used for split data for model fitting is train_test_split function from the sklearn.model_selection module.\n",
    "# Parameters :_\n",
    "# 1, x: Feature dataset\n",
    "# 2, y: Target dataset\n",
    "# 3, Test_size : Proportion of dataset to includein the test split.\n",
    "# 4, Train_size  Proportion of dataset to includein the training split.\n",
    "# 5, random_size : seed for random number generation to ensure reproductivity.\n",
    "# 6, shuffle : whether to shuffle data before spliting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a5580-7bc9-4c0e-b941-10b4f3bbd933",
   "metadata": {},
   "source": [
    "# Q .25 Explain data encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833570d-5c01-4e16-b472-a0357a74975d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Data encoding is the process of converting thecategorical data into a numerical formate so that the machine learning model can process it.\n",
    "\n",
    "types :-\n",
    "# 1, Lable encoding :-\n",
    "#                 Assigns a unique integer to each category.\n",
    "\n",
    "# 2,One-Hot Encoding :-\n",
    "#                 creates binary columns for each categories.\n",
    "\n",
    "# 3,Ordinal Encoding :-\n",
    "#                 Assigns orddered integers based on the category's rank .\n",
    "\n",
    "# 4,Frequency Encoding :-\n",
    "#                 Encoding categories  based on their frequency in the dataset.\n",
    "\n",
    "# 5, Embedding Encoding :-\n",
    "#                 Learns dense vector representations for ctegories, often used woth neural networks.\n",
    "2,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
